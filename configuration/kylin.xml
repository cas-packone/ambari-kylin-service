<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration supports_adding_forbidden="true">
   
  <property>
    <name>kylin_properties</name>
    <description>This is the jinja template for init.conf file</description>
    <value>
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

### METADATA | ENV ###

# The metadata store in hbase
kylin.metadata.url=kylin_metadata@hbase

# Working folder in HDFS, make sure user has the right access to the hdfs directory
kylin.env.hdfs-working-dir=/kylin

# DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.
kylin.env=QA

### SERVER | WEB ###

# Kylin server mode, valid value [all, query, job]
kylin.server.mode={{server_mode}}

# List of web servers in use, this enables one web server instance to sync up with other servers.
kylin.server.cluster-servers={{server_clusters}}

# Display timezone on UI,format like[GMT+N or GMT-N]
kylin.web.timezone=GMT+8

kylin.web.cross-domain-enabled=true


### SOURCE ###

# Hive client, valid value [cli, beeline]
kylin.source.hive.client=cli

# Parameters for beeline client, only necessary if hive client is beeline
#kylin.source.hive.beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://localhost:10000

kylin.source.hive.keep-flat-table=false

# Hive database name for putting the intermediate flat tables
kylin.source.hive.database-for-flat-table=default

# Whether redistribute the intermediate flat table before building
kylin.source.hive.redistribute-flat-table=true

## Configuration for HDP 2.6.x when kylin.source.hive.client=beeline
#kylin.source.hive.sparksql-beeline-shell=/usr/hdp/current/spark2-client/bin/beeline
#kylin.source.hive.sparksql-beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://<hive_host>:10000


### STORAGE ###

# The storage for cube is hbase
kylin.storage.url=hbase

# Compression codec for htable, valid value [none, snappy, lzo, gzip, lz4]
kylin.storage.hbase.compression-codec=none

# HBase Cluster FileSystem, which serving hbase, format as hdfs://hbase-cluster:8020
# Leave empty if hbase running on same cluster with hive and mapreduce
#kylin.storage.hbase.cluster-fs=

# The cut size for hbase region, in GB.
kylin.storage.hbase.region-cut-gb=5

# The hfile size of GB, smaller hfile leading to the converting hfile MR has more reducers and be faster.
# Set 0 to disable this optimization.
kylin.storage.hbase.hfile-size-gb=2

kylin.storage.hbase.min-region-count=1
kylin.storage.hbase.max-region-count=500

# Optional information for the owner of kylin platform, it can be your team's email
# Currently it will be attached to each kylin's htable attribute
kylin.storage.hbase.owner-tag=whoami@kylin.apache.org

kylin.storage.hbase.coprocessor-mem-gb=3

# By default kylin can spill query's intermediate results to disks when it's consuming too much memory.
# Set it to false if you want query to abort immediately in such condition.
kylin.storage.partition.aggr-spill-enabled=true

# The maximum number of bytes each coprocessor is allowed to scan.
# To allow arbitrary large scan, you can set it to 0.
kylin.storage.partition.max-scan-bytes=3221225472

# The default coprocessor timeout is (hbase.rpc.timeout * 0.9) / 1000 seconds,
# You can set it to a smaller value. 0 means use default.
# kylin.storage.hbase.coprocessor-timeout-seconds=0


### JOB ###

# Max job retry on error, default 0: no retry
kylin.job.retry=0

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=10

# The percentage of the sampling, default 100%
kylin.job.sampling-percentage=100

# Whether get job status from resource manager with kerberos authentication
kylin.job.status.with.kerberos=false

# Timeout in seconds
kylin.job.step.timeout=7200

# If true, will send email notification on job complete
#kylin.job.notification-enabled=true
#kylin.job.notification-mail-enable-starttls=true
#kylin.job.notification-mail-host=smtp.office365.com
#kylin.job.notification-mail-port=587
#kylin.job.notification-mail-username=kylin@example.com
#kylin.job.notification-mail-password=mypassword
#kylin.job.notification-mail-sender=kylin@example.com


### ENGINE ###

# Time interval to check hadoop job status
kylin.engine.mr.yarn-check-interval-seconds=10

kylin.engine.mr.reduce-input-mb=500

kylin.engine.mr.max-reducer-number=500

kylin.engine.mr.mapper-input-rows=1000000

# Enable dictionary building in MR reducer
kylin.engine.mr.build-dict-in-reducer=true

# Number of reducers for fetching UHC column distinct values
kylin.engine.mr.uhc-reducer-count=1

### CUBE | DICTIONARY ###

# 'auto', 'inmem' or 'layer'
kylin.cube.algorithm=layer

# A smaller threshold prefers layer, a larger threshold prefers in-mem
kylin.cube.algorithm.layer-or-inmem-threshold=7

kylin.cube.aggrgroup.max-combination=4096

kylin.snapshot.max-mb=300


### QUERY ###

# Controls the maximum number of bytes a query is allowed to scan storage.
# The default value 0 means no limit.
# The counterpart kylin.storage.partition.max-scan-bytes sets the maximum per coprocessor.
kylin.query.max-scan-bytes=0

kylin.query.udf.version=org.apache.kylin.query.udf.VersionUDF
kylin.query.udf.concat=org.apache.kylin.query.udf.ConcatUDF

kylin.query.cache-enabled=true


### SECURITY ###

# Spring security profile, options: testing, ldap, saml
# with "testing" profile, user can use pre-defined name/pwd like KYLIN/ADMIN to login
kylin.security.profile=testing

# Default roles and admin roles in LDAP, for ldap and saml
kylin.security.acl.default-role=ROLE_ANALYST,ROLE_MODELER
kylin.security.acl.admin-role=ROLE_ADMIN

# LDAP authentication configuration
kylin.security.ldap.connection-server=ldap://ldap_server:389
kylin.security.ldap.connection-username=
kylin.security.ldap.connection-password=

# LDAP user account directory;
kylin.security.ldap.user-search-base=
kylin.security.ldap.user-search-pattern=
kylin.security.ldap.user-group-search-base=

# LDAP service account directory
kylin.security.ldap.service-search-base=
kylin.security.ldap.service-search-pattern=
kylin.security.ldap.service-group-search-base=

## SAML configurations for SSO
# SAML IDP metadata file location
kylin.security.saml.metadata-file=classpath:sso_metadata.xml
kylin.security.saml.metadata-entity-base-url=https://hostname/kylin
kylin.security.saml.context-scheme=https
kylin.security.saml.context-server-name=hostname
kylin.security.saml.context-server-port=443
kylin.security.saml.context-path=/kylin


### Spark Engine Configs ###

# Hadoop conf folder, will export this as "HADOOP_CONF_DIR" to run spark-submit
# This must contain site xmls of core, yarn, hive, and hbase in one folder
kylin.env.hadoop-conf-dir={{hadoop_conf_dir}}

# Estimate the RDD partition numbers
kylin.engine.spark.rdd-partition-cut-mb=10

# Minimal partition numbers of rdd
kylin.engine.spark.min-partition=1

# Max partition numbers of rdd
kylin.engine.spark.max-partition=5000

## Spark conf (default is in spark/conf/spark-defaults.conf)
#kylin.engine.spark-conf.spark.master=yarn
#kylin.engine.spark-conf.spark.submit.deployMode=cluster
#kylin.engine.spark-conf.spark.yarn.queue=default
#kylin.engine.spark-conf.spark.executor.memory=4G
#kylin.engine.spark-conf.spark.executor.cores=4
#kylin.engine.spark-conf.spark.executor.instances=2
#kylin.engine.spark-conf.spark.eventLog.enabled=true
#kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///kylin/spark-history
#kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///kylin/spark-history

## Copy the following file /spark/spark-libs.jar from the kylin .tar package to HDFS to avoid uploading dependencies at runtime
#kylin.engine.spark-conf.spark.yarn.archive=hdfs://<namenode_host>:8020/kylin/spark/spark-libs.jar

## or manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime
#kylin.engine.spark-conf.spark.yarn.jar=hdfs://packone165:8020/kylin/spark/spark-assembly-1.6.3-hadoop2.6.0.jar
#kylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec

## uncomment for HDP
#kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version={{hdp_version}}
#kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version={{hdp_version}}
#kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version={{hdp_version}}

    </value>
  </property>
  <property>
    <name>download.location</name>
    <value>https://github.com/alfonsonishikawa/ambari-kylin-service/releases/download/3.1.1/apache-kylin.tar</value>
    <description>Location to download the package Apache Kylin 3.1.1 + spark 2.2.0 + HBase 1.2.0</description>
 </property> 

</configuration>
